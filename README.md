Part of a Student Project at TU Berlin
last change: 13.03.2019

- Main: Code to run the model
- textencoder: [BPE](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization_gpt2.py), The Byte Pair Encoding scheme used by GPT-2

All the data is available as a custom data set downloadable from Kaggle [here](https://www.kaggle.com/flpeters/dstctransformer). 
To run the Model, only data.7z is needed. The other files are the raw sources of text, in case you want to try a different text representation.